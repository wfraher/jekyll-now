---
layout: post
comments: true
title:  "Let's learn reinforcement learning! Part 2: Q-learning"
excerpt: "In this post I describe Q-learning, a popular reinforcement learning algorithm, and how it can be used to "
date:   2020-01-19 11:31:00
mathjax: true
comments: true
---

In my last post, we investigated what reinforcement learning is and how CEM-ES, a simple evolutionary algorithm, can be used to solve certain environments. In this post, we will investigate a technique called Q-learning and its successes.

Q-learning is a reinforcement learning algorithm popularized by Mnih et al. in 2015, with their article *Human-level control through deep reinforcement learning*. These researchers were able to use a neural network to achieve superhuman performance on many Atari 2600 games, based off of only the screen data and scores of these games, without any change to the network's architecture. It achieved a superior performance to the best linear methods on most games.

<img src = "/images/dqn_article/performance.PNG">
(image from Human-level control through deep reinforcement learning, Mnih et al. 2015)

While having been invented by other people, Mnih et al. were the first people to popularize Q-learning with a deep neural network. The goal of Q-learning is to use a neural network to predict the expected rewards from a given state-action pair. Essentially, we want to map from a given state to whichever action will yield the highest expected sum of rewards, using a neural network. 

$\pi_\theta(s_t) = max_a(Q(s_t,a_t))$

Where $Q(s_t,a_t)$ is the value of an action $a_t$ at a state $s_t$ given we're at time step t, so $Q(s_t,a_t)$ is the value of the state-action pair. The values of state-action pairs are called q-values. We want to build a neural network that maps from states to expected values for each action, so our neural network will approximate $Q(s_t,a_t)$. We will test out how q-learning performs on the pole balancing problem before moving to something more complicated.

It might seem that we would want our neural network to predict the following target:

$Q(s_t,a_t) = \mathbb{E}[r_t + r_{t+1} + r_{t+2} ...]$

essentially predicting the sum of all rewards throughout the whole training episode. However, the action the agent currently takes might not be what really warrants a larger reward later on in the episode. As a result, we can introduce another parameter, $\gamma$, to give preference to more recent rewards in comparison to rewards later on. This makes the target modified as such:

$Q(s,a) = \mathbb{E}[r_t + \gamma * r_{t+1} + \gamma^2 * r_{t+2} ...]$

We want $\gamma$ to be less than 1. In reality, q-learning with a neural network can be pretty unstable, so we prefer a target that makes use of our own q-value approximator:

$Q(s_t,a_t) = r_t + \gamma * max_{a_{t+1}}(Q_{t+1},a_{t+1})$

We will train a neural network to predict this target value using values randomly sampled from the agent's performance. To do this, let's first define a memory buffer to easily load and store values:

```python

#Defining the memory module

class Memory():

  def __init__(self):
    self.data = []

  def push(self,new):

    self.data.append(new)

    if len(self.data) > capacity:
      amount = len(self.data) - capacity
      self.data[0:amount] = []
    
  def __len__(self):
    return len(self.data)

  def sample(self):
    new_data = random.sample(self.data,batch_size)
    return new_data
    
```

We will use PyTorch to define and train the neural network. PyTorch is a framework for neural networks and automatic differentiation which allows us to build neural networks quickly. [Here are PyTorch's official tutorials if you're not familiar with it.](https://pytorch.org/tutorials/)

Our model will need PyTorch tensors to be fed into it, which we can accomplish with the following preprocessing method:

```python
def preprocess(states):
  return torch.FloatTensor(states)
```

I chose to leave this as a function for the purposes of routine; for more complicated environments we may need more preprocessing. State size and action size are defined with `env.observation_space.shape[0]` and `env.action_space.n` like last time.

I chose the following hyperparameters as well:

```python
lr = 0.0005
max_steps = 500 #maximum for cartpole_v1
episodes = 200
discount = 0.95
hidden_size = 32
batch_size = 32
print_outputs = True
```

We'll use a learning rate of 5e-4. 0.95 will be the discount rate, and importantly, 32 will be the size of our hidden layer for a simple neural network. Another thing to consider when using q-learning is that we want to add an element of randomness to our agent. If our agent has no randomness, it will keep doing the same thing every time and depend on randomness from the environment. However, if it sometimes acts randomly, it will explore new options. Additionally, if it starts out making moves entirely at random, the neural network will be exposed to the variety of episodes generated by a random agent rather than the more deterministic nature of a neural network. We will accomplish this with a parameter called epsilon.

```python
#DQN-specific parameters

capacity = 2000
epsilon_start = 0.99
epsilon_decay = 1000
epsilon_min = 0.1
epsilon = epsilon_start
```

In this block of code we also define the maximal amount of episodes we will train off of. We will select episodes at random to train the neural network. We don't want to hang on to any episodes forever or else we risk overfitting. Thus it is practical to remove old episodes, once we have the amount dictated by the capacity parameter, before we add new ones.

```python
#Defining the neural network

def initializer(param):
  torch.nn.init.xavier_uniform(param)

class Net(nn.Module):

    def __init__(self):

        #Pytorch has bias layers built in. In tensorflow, we might add a bias layer separately.
        
        super(Net, self).__init__()

        #Instantiates and initializes the hidden layer
        self.hidden_layer = nn.Linear(state_size,hidden_size)
        initializer(self.hidden_layer.weight)

        #Instantiates and initializes the layer that computes state-action values
        self.action_layer = nn.Linear(hidden_size,action_size)
        initializer(self.action_layer.weight)

    def forward(self,x):

        #X is a state for which the network computes state-action values
        x = F.relu(self.hidden_layer(x))
        x = self.action_layer(x)
        return x
```

Our neural network here is a simple model that uses one hidden layer, with size determined by the hidden_size parameter. Let's define a training function as follows:

```python
#Training loop

def train():

    sample = memory.sample()
    states,actions,rewards,nextstates,done = zip(*sample)


    states = torch.stack(states)
    nextstates = torch.stack(nextstates)
    actions = torch.stack(actions).view(batch_size,1)
    rewards = torch.tensor(rewards)

    q_values = net(states).gather(1,actions.long())#torch.sum(net(states) * torch.FloatTensor(one_hot_actions,device=device).unsqueeze(1).detach(),dim=2)

    with torch.no_grad():
        target = rewards + discount*(net(nextstates).max(1)[0].detach()) + (-1 * torch.FloatTensor(done) - 1)

    optimizer.zero_grad()
    loss = criterion(q_values,target.unsqueeze(1))
    loss.backward()
    for param in net.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()
```

We have to instantiate the network and memory buffer. Let's also set up a list to store how well the agent performs over time, as well as the loss function and optimizer. I have chosen to use mean squared error loss here. Huber loss is also effective, but I have had [better results using mean squared error loss for the pole balancing problem.](https://github.com/wfraher/PyTorch-Vanilla-DQN-Experiment)

```python
#Execution and training loop
net = Net()
memory = Memory()
total_rewards = []

#Defining the optimizer and loss function

criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=lr)
```

Now we can iterate over each of the episodes, with a maximal number already established, using the following procedure:

```python

for i in range(episodes):

  state = preprocess(env.reset())
  done = False
  episode_reward = 0 #running reward for the episode

  for t in range(max_steps):

    with torch.no_grad():
      if epsilon < np.random.rand():
        action = net(state).argmax(0)
      else:
        action = torch.tensor(np.random.randint(0,action_size))
        epsilon_rate = (epsilon_start - epsilon_min) / epsilon_decay
        epsilon = max(epsilon_min, epsilon - epsilon_rate)

    nextState, reward, done, _ = env.step(action.item())
    nextState = preprocess(nextState)
    episode_reward += reward
    memory.push((state, action, reward, nextState, done))
    state = nextState
    
    if done:
      if print_outputs:
        print('Episode ' + str(i) + ' finished with reward ' + str(episode_reward) + ' with epsilon ' + str(epsilon))
      total_rewards.append(episode_reward)
      break

    if len(memory) >= batch_size:

      train()
     
```

Once training is complete, we graph the results with the following code:

```python
#Graph results

print("Average loss: " + str(sum(total_rewards)/len(total_rewards)))
print("Final training episode graph:")
plt.plot(total_rewards)
plt.show()
```

This warranted the following results when I ran it:

<img src = "/images/dqn_article/cartpole_performance.png">

which solves the environment, though it is unstable. I tested out mean squared error loss against huber loss, and noted that while [mean squared error loss has a higher average score for this environment, huber loss is more stable.](https://github.com/wfraher/PyTorch-Vanilla-DQN-Experiment) If you'd like to try huber loss for this agent, change

```python
loss = criterion(q_values,target.unsqueeze(1))
```

to

```python
loss = F.smooth_l1_loss(q_values,target.unsqueeze(1))
```

and see for yourself. Thank you for reading this, I wish you luck with reinforcement learning and all endeavors. Soon I will cover improvements to q-learning which will make it more powerful.
